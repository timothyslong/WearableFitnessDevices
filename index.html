<html>

<head>
<style type="text/css">
.knitr .inline {
  background-color: #f7f7f7;
  border:solid 1px #B0B0B0;
}
.error {
	font-weight: bold;
	color: #FF0000;
}
.warning {
	font-weight: bold;
}
.message {
	font-style: italic;
}
.source, .output, .warning, .error, .message {
	padding: 0 1em;
  border:solid 1px #F7F7F7;
}
.source {
  background-color: #f5f5f5;
}
.rimage .left {
  text-align: left;
}
.rimage .right {
  text-align: right;
}
.rimage .center {
  text-align: center;
}
.hl.num {
  color: #AF0F91;
}
.hl.str {
  color: #317ECC;
}
.hl.com {
  color: #AD95AF;
  font-style: italic;
}
.hl.opt {
  color: #000000;
}
.hl.std {
  color: #585858;
}
.hl.kwa {
  color: #295F94;
  font-weight: bold;
}
.hl.kwb {
  color: #B05A65;
}
.hl.kwc {
  color: #55aa55;
}
.hl.kwd {
  color: #BC5A65;
  font-weight: bold;
}
</style>
  <title>Wearable fitness devices have profound ability to predict proper weight lifting techniques.</title>
</head>

<body>

<h1>Wearable fitness devices collect data that can predict proper weight lifting technique with 99.95% accuracy.</h1>

<h2>Executive Summary:</h2>
<p>
A Gradient Boosting Machine (gbm) is trained using wearable fitness device sensor data to predict whether weight lifting techniques are being performed properly.  A trained model is able to achieve 99.95% accuracy predicting the five different outcomes (A = Good Technique; B,C,D,E = Bad Techniques).
</p>




<h4>Data Preparation Techniques:</h4>
<ol>
  <li>19,622 observations of 155 variables were loaded from the source</li>
  <li>Row names (X), user_name, and all timestamps were removed as predictors to minimize overfitting</li>
  <li>Training and Testing sets were created using a 70% training random sampling from the overall set</li>
  <li>To improve efficiency of model generation without losing predictive capabilities, 77 variables with near zero variance were removed from the training set</li>
  <li>KNN Imputation was used to impute values that were loaded as NA (in cases primarily where values were #DIV/0! and thus columns were converted to factors)</li>
  <li>Because tree-based methods are a good fit for this multi-class non-linear classification problem, no other monotonic transformations were used</li>
</ol>

<h4>Exploration of Training Data:</h4>
<p>Initial data exploration shows many features with significant correlations to each class (A, B, C, D, E):</p>

<div class="chunk" id="unnamed-chunk-2"><div class="rimage default"><img src="figure/unnamed-chunk-2-1.png" title="plot of chunk unnamed-chunk-2" alt="plot of chunk unnamed-chunk-2" class="plot" /></div></div>

<h4>Training the Predictive Algorithms</h4>
<p>Three different classification algorithms were tested with varying results: Decision Trees (rpart), Random Forests, and Gradient Boosting Machine (boosted trees).  In each trained algorithm, repeated 10-fold cross-validation was performed to minimize overfitting while optimizing tuning parameters for maximum classification accuracy.</p>

<p>The best performing algorithm was created using <b>Gradient Boosting Machine (gbm)</b> package which combines boosting and trees to achieve an outstanding <b>99.95% overall class prediction accuracy</b> on test data.</p>



<h4>Tuning the Algorithm for Maximum Accuracy</h4>
<p>Through experimentation, excellent tuning parameters were found using 10-fold cross-validation with 10 repeats.  These parameters include 500 trees with an interaction depth of 7 levels, shrinkage of 0.1, and 50 minimum observations in each leaf node.</p>

<div class="chunk" id="unnamed-chunk-4"><div class="rimage default"><img src="figure/unnamed-chunk-4-1.png" title="plot of chunk unnamed-chunk-4" alt="plot of chunk unnamed-chunk-4" class="plot" /></div></div>

<p>Variables with non-zero effects in the final predictive model are plotted below (from most influential to least).</p>

<div class="chunk" id="unnamed-chunk-5"><div class="rimage default"><img src="figure/unnamed-chunk-5-1.png" title="plot of chunk unnamed-chunk-5" alt="plot of chunk unnamed-chunk-5" class="plot" /></div></div>


<h4>Results from the Testing Dataset: Confusion Matrix and Summary of Accuracy</h4>

<div class="chunk" id="unnamed-chunk-6"><div class="rcode"><div class="output"><pre class="knitr r">## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1674    0    0    0    0
##          B    0 1139    0    0    1
##          C    0    0 1026    0    0
##          D    0    0    0  964    2
##          E    0    0    0    0 1079
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9995          
##                  95% CI : (0.9985, 0.9999)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9994          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            1.0000   1.0000   1.0000   1.0000   0.9972
## Specificity            1.0000   0.9998   1.0000   0.9996   1.0000
## Pos Pred Value         1.0000   0.9991   1.0000   0.9979   1.0000
## Neg Pred Value         1.0000   1.0000   1.0000   1.0000   0.9994
## Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
## Detection Rate         0.2845   0.1935   0.1743   0.1638   0.1833
## Detection Prevalence   0.2845   0.1937   0.1743   0.1641   0.1833
## Balanced Accuracy      1.0000   0.9999   1.0000   0.9998   0.9986
</pre></div>
</div></div>





</body>
</html>
